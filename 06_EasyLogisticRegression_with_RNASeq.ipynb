{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"g5TEDU_Tt4qX"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD, Adam\n","\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/ark1st/tuto_AI/main/dataset/gene.csv"],"metadata":{"id":"n2UqxGKhAIsT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"/content/gene.csv\", header=None)"],"metadata":{"id":"rydYK0OHu2nF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = df.iloc[:, 0:52]  # 0에서 53번째 열을 X로 저장\n","Y = df.iloc[:, 53]   # 54번째 열을 Y로 저장"],"metadata":{"id":"zwhBnH3xPyY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=138)"],"metadata":{"id":"KJaYTOUiNE_8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#순차적인 모델을 만들것이다. 위에서부터 아래로 쌓는다.\n","model = Sequential()\n","\n","#Dense Layer : 모든 뉴런마다 인풋이 전부 연결된 레이어\n","#Output 1개, input도 1개\n","model.add(Dense(1, input_shape=(52,), activation = 'sigmoid'))\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xytt-os9u2jI","outputId":"f5337bc8-b3ae-4bc3-8c18-aca78dd0000a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_10 (Dense)            (None, 1)                 53        \n","                                                                 \n","=================================================================\n","Total params: 53 (212.00 Byte)\n","Trainable params: 53 (212.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(optimizer = Adam(lr=0.001), loss = 'binary_crossentropy', metrics=['acc'])\n","\n","model.fit(X_train, Y_train, epochs=100, batch_size=128, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpr8uncmvoeG","outputId":"858d7c3d-5b2c-4887-93b6-5c530317ecf5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","42/42 [==============================] - 1s 8ms/step - loss: 2.6480 - acc: 0.5778 - val_loss: 1.2400 - val_acc: 0.5649\n","Epoch 2/100\n","42/42 [==============================] - 0s 4ms/step - loss: 1.1417 - acc: 0.5539 - val_loss: 1.0538 - val_acc: 0.5485\n","Epoch 3/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.9798 - acc: 0.6047 - val_loss: 0.9234 - val_acc: 0.6000\n","Epoch 4/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.8592 - acc: 0.6416 - val_loss: 0.8125 - val_acc: 0.6358\n","Epoch 5/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.7608 - acc: 0.6724 - val_loss: 0.7216 - val_acc: 0.6709\n","Epoch 6/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.6781 - acc: 0.6993 - val_loss: 0.6562 - val_acc: 0.6918\n","Epoch 7/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.6119 - acc: 0.7211 - val_loss: 0.5866 - val_acc: 0.7291\n","Epoch 8/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.5564 - acc: 0.7455 - val_loss: 0.5408 - val_acc: 0.7500\n","Epoch 9/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.5130 - acc: 0.7636 - val_loss: 0.5058 - val_acc: 0.7746\n","Epoch 10/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4766 - acc: 0.7845 - val_loss: 0.4719 - val_acc: 0.7948\n","Epoch 11/100\n","42/42 [==============================] - 0s 9ms/step - loss: 0.4478 - acc: 0.7981 - val_loss: 0.4465 - val_acc: 0.8045\n","Epoch 12/100\n","42/42 [==============================] - 0s 8ms/step - loss: 0.4230 - acc: 0.8140 - val_loss: 0.4239 - val_acc: 0.8142\n","Epoch 13/100\n","42/42 [==============================] - 0s 10ms/step - loss: 0.4024 - acc: 0.8271 - val_loss: 0.4057 - val_acc: 0.8246\n","Epoch 14/100\n","42/42 [==============================] - 1s 15ms/step - loss: 0.3859 - acc: 0.8373 - val_loss: 0.3926 - val_acc: 0.8358\n","Epoch 15/100\n","42/42 [==============================] - 0s 8ms/step - loss: 0.3697 - acc: 0.8472 - val_loss: 0.3776 - val_acc: 0.8425\n","Epoch 16/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3556 - acc: 0.8535 - val_loss: 0.3638 - val_acc: 0.8470\n","Epoch 17/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.3440 - acc: 0.8629 - val_loss: 0.3516 - val_acc: 0.8552\n","Epoch 18/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.3325 - acc: 0.8675 - val_loss: 0.3412 - val_acc: 0.8597\n","Epoch 19/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.3229 - acc: 0.8767 - val_loss: 0.3317 - val_acc: 0.8679\n","Epoch 20/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.3133 - acc: 0.8830 - val_loss: 0.3225 - val_acc: 0.8754\n","Epoch 21/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.3051 - acc: 0.8856 - val_loss: 0.3153 - val_acc: 0.8806\n","Epoch 22/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2966 - acc: 0.8910 - val_loss: 0.3090 - val_acc: 0.8843\n","Epoch 23/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2893 - acc: 0.8927 - val_loss: 0.3001 - val_acc: 0.8896\n","Epoch 24/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2827 - acc: 0.8953 - val_loss: 0.2946 - val_acc: 0.8925\n","Epoch 25/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2762 - acc: 0.9000 - val_loss: 0.2872 - val_acc: 0.8955\n","Epoch 26/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2709 - acc: 0.9013 - val_loss: 0.2817 - val_acc: 0.8993\n","Epoch 27/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2648 - acc: 0.9056 - val_loss: 0.2759 - val_acc: 0.9045\n","Epoch 28/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2603 - acc: 0.9067 - val_loss: 0.2727 - val_acc: 0.9022\n","Epoch 29/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2551 - acc: 0.9108 - val_loss: 0.2681 - val_acc: 0.9030\n","Epoch 30/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2504 - acc: 0.9112 - val_loss: 0.2614 - val_acc: 0.9090\n","Epoch 31/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2464 - acc: 0.9132 - val_loss: 0.2570 - val_acc: 0.9104\n","Epoch 32/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.2422 - acc: 0.9153 - val_loss: 0.2531 - val_acc: 0.9112\n","Epoch 33/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2387 - acc: 0.9198 - val_loss: 0.2502 - val_acc: 0.9119\n","Epoch 34/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2351 - acc: 0.9201 - val_loss: 0.2469 - val_acc: 0.9142\n","Epoch 35/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2313 - acc: 0.9200 - val_loss: 0.2419 - val_acc: 0.9127\n","Epoch 36/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2280 - acc: 0.9241 - val_loss: 0.2390 - val_acc: 0.9172\n","Epoch 37/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2255 - acc: 0.9265 - val_loss: 0.2351 - val_acc: 0.9157\n","Epoch 38/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.2223 - acc: 0.9248 - val_loss: 0.2335 - val_acc: 0.9194\n","Epoch 39/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2193 - acc: 0.9263 - val_loss: 0.2299 - val_acc: 0.9179\n","Epoch 40/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2165 - acc: 0.9267 - val_loss: 0.2266 - val_acc: 0.9209\n","Epoch 41/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2141 - acc: 0.9295 - val_loss: 0.2252 - val_acc: 0.9216\n","Epoch 42/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2113 - acc: 0.9289 - val_loss: 0.2220 - val_acc: 0.9209\n","Epoch 43/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2091 - acc: 0.9300 - val_loss: 0.2205 - val_acc: 0.9261\n","Epoch 44/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2076 - acc: 0.9293 - val_loss: 0.2166 - val_acc: 0.9224\n","Epoch 45/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.2053 - acc: 0.9312 - val_loss: 0.2164 - val_acc: 0.9261\n","Epoch 46/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2025 - acc: 0.9315 - val_loss: 0.2142 - val_acc: 0.9261\n","Epoch 47/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.2008 - acc: 0.9321 - val_loss: 0.2113 - val_acc: 0.9261\n","Epoch 48/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1982 - acc: 0.9334 - val_loss: 0.2083 - val_acc: 0.9269\n","Epoch 49/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1964 - acc: 0.9340 - val_loss: 0.2087 - val_acc: 0.9261\n","Epoch 50/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1946 - acc: 0.9330 - val_loss: 0.2065 - val_acc: 0.9261\n","Epoch 51/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1931 - acc: 0.9353 - val_loss: 0.2096 - val_acc: 0.9216\n","Epoch 52/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1930 - acc: 0.9340 - val_loss: 0.2009 - val_acc: 0.9291\n","Epoch 53/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1903 - acc: 0.9349 - val_loss: 0.1990 - val_acc: 0.9299\n","Epoch 54/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1880 - acc: 0.9368 - val_loss: 0.1976 - val_acc: 0.9284\n","Epoch 55/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1864 - acc: 0.9360 - val_loss: 0.1983 - val_acc: 0.9269\n","Epoch 56/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1849 - acc: 0.9362 - val_loss: 0.1949 - val_acc: 0.9299\n","Epoch 57/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1836 - acc: 0.9371 - val_loss: 0.1938 - val_acc: 0.9306\n","Epoch 58/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1819 - acc: 0.9381 - val_loss: 0.1933 - val_acc: 0.9299\n","Epoch 59/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1807 - acc: 0.9373 - val_loss: 0.1909 - val_acc: 0.9299\n","Epoch 60/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1788 - acc: 0.9386 - val_loss: 0.1909 - val_acc: 0.9291\n","Epoch 61/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1792 - acc: 0.9384 - val_loss: 0.1879 - val_acc: 0.9328\n","Epoch 62/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1768 - acc: 0.9384 - val_loss: 0.1861 - val_acc: 0.9321\n","Epoch 63/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1757 - acc: 0.9396 - val_loss: 0.1867 - val_acc: 0.9306\n","Epoch 64/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1741 - acc: 0.9396 - val_loss: 0.1835 - val_acc: 0.9351\n","Epoch 65/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1728 - acc: 0.9396 - val_loss: 0.1824 - val_acc: 0.9351\n","Epoch 66/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1726 - acc: 0.9399 - val_loss: 0.1909 - val_acc: 0.9269\n","Epoch 67/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1710 - acc: 0.9405 - val_loss: 0.1811 - val_acc: 0.9343\n","Epoch 68/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1696 - acc: 0.9409 - val_loss: 0.1805 - val_acc: 0.9351\n","Epoch 69/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1689 - acc: 0.9379 - val_loss: 0.1788 - val_acc: 0.9403\n","Epoch 70/100\n","42/42 [==============================] - 0s 7ms/step - loss: 0.1676 - acc: 0.9412 - val_loss: 0.1777 - val_acc: 0.9351\n","Epoch 71/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.1665 - acc: 0.9414 - val_loss: 0.1761 - val_acc: 0.9366\n","Epoch 72/100\n","42/42 [==============================] - 0s 7ms/step - loss: 0.1652 - acc: 0.9409 - val_loss: 0.1748 - val_acc: 0.9381\n","Epoch 73/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.1650 - acc: 0.9414 - val_loss: 0.1797 - val_acc: 0.9313\n","Epoch 74/100\n","42/42 [==============================] - 0s 7ms/step - loss: 0.1640 - acc: 0.9433 - val_loss: 0.1726 - val_acc: 0.9381\n","Epoch 75/100\n","42/42 [==============================] - 0s 7ms/step - loss: 0.1628 - acc: 0.9424 - val_loss: 0.1715 - val_acc: 0.9388\n","Epoch 76/100\n","42/42 [==============================] - 0s 7ms/step - loss: 0.1620 - acc: 0.9429 - val_loss: 0.1724 - val_acc: 0.9366\n","Epoch 77/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.1608 - acc: 0.9420 - val_loss: 0.1698 - val_acc: 0.9410\n","Epoch 78/100\n","42/42 [==============================] - 0s 7ms/step - loss: 0.1603 - acc: 0.9418 - val_loss: 0.1698 - val_acc: 0.9381\n","Epoch 79/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1592 - acc: 0.9418 - val_loss: 0.1683 - val_acc: 0.9418\n","Epoch 80/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1586 - acc: 0.9446 - val_loss: 0.1722 - val_acc: 0.9366\n","Epoch 81/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1586 - acc: 0.9433 - val_loss: 0.1664 - val_acc: 0.9448\n","Epoch 82/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1565 - acc: 0.9437 - val_loss: 0.1664 - val_acc: 0.9388\n","Epoch 83/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1561 - acc: 0.9448 - val_loss: 0.1653 - val_acc: 0.9388\n","Epoch 84/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1551 - acc: 0.9440 - val_loss: 0.1642 - val_acc: 0.9440\n","Epoch 85/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1547 - acc: 0.9461 - val_loss: 0.1643 - val_acc: 0.9418\n","Epoch 86/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1536 - acc: 0.9442 - val_loss: 0.1638 - val_acc: 0.9418\n","Epoch 87/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1532 - acc: 0.9457 - val_loss: 0.1621 - val_acc: 0.9448\n","Epoch 88/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1520 - acc: 0.9451 - val_loss: 0.1621 - val_acc: 0.9418\n","Epoch 89/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1515 - acc: 0.9461 - val_loss: 0.1605 - val_acc: 0.9448\n","Epoch 90/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1509 - acc: 0.9450 - val_loss: 0.1607 - val_acc: 0.9418\n","Epoch 91/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1508 - acc: 0.9457 - val_loss: 0.1595 - val_acc: 0.9463\n","Epoch 92/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1498 - acc: 0.9461 - val_loss: 0.1584 - val_acc: 0.9463\n","Epoch 93/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1486 - acc: 0.9472 - val_loss: 0.1597 - val_acc: 0.9403\n","Epoch 94/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1493 - acc: 0.9493 - val_loss: 0.1578 - val_acc: 0.9425\n","Epoch 95/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1480 - acc: 0.9474 - val_loss: 0.1563 - val_acc: 0.9463\n","Epoch 96/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.1471 - acc: 0.9481 - val_loss: 0.1567 - val_acc: 0.9433\n","Epoch 97/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1476 - acc: 0.9468 - val_loss: 0.1549 - val_acc: 0.9478\n","Epoch 98/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1459 - acc: 0.9485 - val_loss: 0.1549 - val_acc: 0.9448\n","Epoch 99/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1454 - acc: 0.9498 - val_loss: 0.1581 - val_acc: 0.9403\n","Epoch 100/100\n","42/42 [==============================] - 0s 4ms/step - loss: 0.1457 - acc: 0.9485 - val_loss: 0.1546 - val_acc: 0.9425\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7d4a10cdeb30>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["Y_pred = model.predict(X_test)\n","Y_pred = Y_pred.round()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"asPIejGrXm96","outputId":"05f1afaf-b408-4dcb-9010-99f38642dbfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["104/104 [==============================] - 0s 2ms/step\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(Y_test, Y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BH3qkAegXp4G","outputId":"88827905-2565-4691-f80e-e79b8c08d63b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      0.94      0.94      1388\n","           1       0.96      0.96      0.96      1912\n","\n","    accuracy                           0.95      3300\n","   macro avg       0.95      0.95      0.95      3300\n","weighted avg       0.95      0.95      0.95      3300\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JntPuIpYXzgu"},"execution_count":null,"outputs":[]}]}